{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\n\n# Generate toy data (2D points, binary classification)\ndef generate_data(n_samples=1000):\n    X = np.random.randn(n_samples, 2)\n    y = (X[:, 0] * X[:, 1] > 0).astype(int)  # Class 1 if x*y > 0, else Class 0\n    return X, y.reshape(-1, 1)\n\n# Activation functions and their derivatives\ndef relu(x):\n    return np.maximum(0, x)\n\ndef relu_derivative(x):\n    return (x > 0).astype(float)\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    s = sigmoid(x)\n    return s * (1 - s)\n\n# Loss and performance\ndef mse_loss(y_true, y_pred):\n    return np.mean((y_true - y_pred) ** 2)\n\ndef accuracy(y_true, y_pred):\n    pred_labels = (y_pred > 0.5).astype(int)\n    return np.mean(pred_labels == y_true)\n\n# Neural Network\nclass SimpleNN:\n    def __init__(self, input_size, hidden_size, output_size, lr=0.01):\n        self.lr = lr\n        self.w1 = np.random.randn(input_size, hidden_size) * 0.1\n        self.b1 = np.zeros((1, hidden_size))\n        self.w2 = np.random.randn(hidden_size, output_size) * 0.1\n        self.b2 = np.zeros((1, output_size))\n        \n\n    def forward(self, X):\n        self.z1 = X @ self.w1 + self.b1\n        self.a1 = relu(self.z1)\n        self.z2 = self.a1 @ self.w2 + self.b2\n        self.a2 = sigmoid(self.z2)\n        return self.a2\n\n    def backward(self, X, y, y_pred):\n        m = X.shape[0]\n        dz2 = (y_pred - y) * sigmoid_derivative(self.z2)\n        dw2 = self.a1.T @ dz2 / m\n        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n\n        dz1 = dz2 @ self.w2.T * relu_derivative(self.z1)\n        dw1 = X.T @ dz1 / m\n        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n\n        # Update weights\n        self.w2 -= self.lr * dw2\n        self.b2 -= self.lr * db2\n        self.w1 -= self.lr * dw1\n        self.b1 -= self.lr * db1\n\n    def train(self, X, y, epochs=100):\n        for epoch in range(epochs):\n            y_pred = self.forward(X)\n            loss = mse_loss(y, y_pred)\n            self.backward(X, y, y_pred)\n\n            if epoch % 10 == 0:\n                acc = accuracy(y, y_pred)\n                print(f\"Epoch {epoch}: Loss = {loss:.4f}, Accuracy = {acc:.4f}\")\n\n# Run\nX, y = generate_data(1000)\n\nmodel = SimpleNN(input_size=2, hidden_size=50, output_size=1, lr=0.1)\nmodel.train(X, y, epochs=800)\n\n# Final performance\ny_pred = model.forward(X)\nprint(\"\\nFinal Accuracy:\", accuracy(y, y_pred))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:38:04.806297Z","iopub.execute_input":"2025-05-10T13:38:04.806599Z","iopub.status.idle":"2025-05-10T13:38:05.552201Z","shell.execute_reply.started":"2025-05-10T13:38:04.806574Z","shell.execute_reply":"2025-05-10T13:38:05.551492Z"}},"outputs":[{"name":"stdout","text":"Epoch 0: Loss = 0.2470, Accuracy = 0.6080\nEpoch 10: Loss = 0.2454, Accuracy = 0.5820\nEpoch 20: Loss = 0.2438, Accuracy = 0.5700\nEpoch 30: Loss = 0.2422, Accuracy = 0.6030\nEpoch 40: Loss = 0.2407, Accuracy = 0.6650\nEpoch 50: Loss = 0.2391, Accuracy = 0.7120\nEpoch 60: Loss = 0.2375, Accuracy = 0.7250\nEpoch 70: Loss = 0.2360, Accuracy = 0.7380\nEpoch 80: Loss = 0.2345, Accuracy = 0.7470\nEpoch 90: Loss = 0.2331, Accuracy = 0.7510\nEpoch 100: Loss = 0.2317, Accuracy = 0.7490\nEpoch 110: Loss = 0.2303, Accuracy = 0.7500\nEpoch 120: Loss = 0.2289, Accuracy = 0.7510\nEpoch 130: Loss = 0.2275, Accuracy = 0.7570\nEpoch 140: Loss = 0.2262, Accuracy = 0.7600\nEpoch 150: Loss = 0.2248, Accuracy = 0.7630\nEpoch 160: Loss = 0.2235, Accuracy = 0.7680\nEpoch 170: Loss = 0.2221, Accuracy = 0.7730\nEpoch 180: Loss = 0.2208, Accuracy = 0.7850\nEpoch 190: Loss = 0.2194, Accuracy = 0.7900\nEpoch 200: Loss = 0.2181, Accuracy = 0.7970\nEpoch 210: Loss = 0.2167, Accuracy = 0.7990\nEpoch 220: Loss = 0.2153, Accuracy = 0.8080\nEpoch 230: Loss = 0.2139, Accuracy = 0.8120\nEpoch 240: Loss = 0.2125, Accuracy = 0.8180\nEpoch 250: Loss = 0.2111, Accuracy = 0.8260\nEpoch 260: Loss = 0.2096, Accuracy = 0.8300\nEpoch 270: Loss = 0.2082, Accuracy = 0.8350\nEpoch 280: Loss = 0.2067, Accuracy = 0.8450\nEpoch 290: Loss = 0.2052, Accuracy = 0.8490\nEpoch 300: Loss = 0.2037, Accuracy = 0.8540\nEpoch 310: Loss = 0.2022, Accuracy = 0.8590\nEpoch 320: Loss = 0.2006, Accuracy = 0.8630\nEpoch 330: Loss = 0.1991, Accuracy = 0.8650\nEpoch 340: Loss = 0.1975, Accuracy = 0.8750\nEpoch 350: Loss = 0.1959, Accuracy = 0.8800\nEpoch 360: Loss = 0.1943, Accuracy = 0.8860\nEpoch 370: Loss = 0.1927, Accuracy = 0.8920\nEpoch 380: Loss = 0.1911, Accuracy = 0.8970\nEpoch 390: Loss = 0.1895, Accuracy = 0.8990\nEpoch 400: Loss = 0.1878, Accuracy = 0.9050\nEpoch 410: Loss = 0.1862, Accuracy = 0.9070\nEpoch 420: Loss = 0.1846, Accuracy = 0.9130\nEpoch 430: Loss = 0.1829, Accuracy = 0.9150\nEpoch 440: Loss = 0.1813, Accuracy = 0.9180\nEpoch 450: Loss = 0.1796, Accuracy = 0.9180\nEpoch 460: Loss = 0.1779, Accuracy = 0.9220\nEpoch 470: Loss = 0.1763, Accuracy = 0.9270\nEpoch 480: Loss = 0.1746, Accuracy = 0.9290\nEpoch 490: Loss = 0.1730, Accuracy = 0.9310\nEpoch 500: Loss = 0.1713, Accuracy = 0.9350\nEpoch 510: Loss = 0.1697, Accuracy = 0.9370\nEpoch 520: Loss = 0.1681, Accuracy = 0.9370\nEpoch 530: Loss = 0.1664, Accuracy = 0.9380\nEpoch 540: Loss = 0.1648, Accuracy = 0.9420\nEpoch 550: Loss = 0.1632, Accuracy = 0.9440\nEpoch 560: Loss = 0.1616, Accuracy = 0.9450\nEpoch 570: Loss = 0.1600, Accuracy = 0.9470\nEpoch 580: Loss = 0.1584, Accuracy = 0.9490\nEpoch 590: Loss = 0.1569, Accuracy = 0.9510\nEpoch 600: Loss = 0.1553, Accuracy = 0.9560\nEpoch 610: Loss = 0.1538, Accuracy = 0.9570\nEpoch 620: Loss = 0.1523, Accuracy = 0.9590\nEpoch 630: Loss = 0.1508, Accuracy = 0.9600\nEpoch 640: Loss = 0.1493, Accuracy = 0.9610\nEpoch 650: Loss = 0.1478, Accuracy = 0.9610\nEpoch 660: Loss = 0.1463, Accuracy = 0.9620\nEpoch 670: Loss = 0.1449, Accuracy = 0.9630\nEpoch 680: Loss = 0.1435, Accuracy = 0.9640\nEpoch 690: Loss = 0.1421, Accuracy = 0.9690\nEpoch 700: Loss = 0.1407, Accuracy = 0.9690\nEpoch 710: Loss = 0.1393, Accuracy = 0.9690\nEpoch 720: Loss = 0.1380, Accuracy = 0.9690\nEpoch 730: Loss = 0.1366, Accuracy = 0.9690\nEpoch 740: Loss = 0.1353, Accuracy = 0.9700\nEpoch 750: Loss = 0.1340, Accuracy = 0.9720\nEpoch 760: Loss = 0.1327, Accuracy = 0.9720\nEpoch 770: Loss = 0.1315, Accuracy = 0.9730\nEpoch 780: Loss = 0.1303, Accuracy = 0.9730\nEpoch 790: Loss = 0.1290, Accuracy = 0.9730\n\nFinal Accuracy: 0.973\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"new_data, y = generate_data(10)\npred_probs = model.forward(new_data)\npred_classes = (pred_probs > 0.5).astype(int)\nprint(\"Predicted classes:\", list(zip(y, pred_classes.ravel())))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:38:38.797551Z","iopub.execute_input":"2025-05-10T13:38:38.797855Z","iopub.status.idle":"2025-05-10T13:38:38.803589Z","shell.execute_reply.started":"2025-05-10T13:38:38.797831Z","shell.execute_reply":"2025-05-10T13:38:38.802798Z"}},"outputs":[{"name":"stdout","text":"Predicted classes: [(array([0]), 0), (array([0]), 0), (array([0]), 0), (array([0]), 0), (array([0]), 0), (array([0]), 0), (array([0]), 0), (array([0]), 0), (array([1]), 1), (array([1]), 1)]\n","output_type":"stream"}],"execution_count":8}]}